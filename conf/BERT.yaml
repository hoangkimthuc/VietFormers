model:
  emb_dim: 200
  max_seq_len: 512
  num_attention_heads: 2
  num_encoder_blocks: 2
  hidden_size: 20
  num_layers: 2
  dropout_p: 0.2
training:
  train_batch_size: 20
  eval_batch_size: 10
  sq_len: 35
  num_epochs: 1
  lr: 5
  log_interval: 1
wandb:
  is_enabled: True
  project: VietFormers
  run_name: training_BERT
  watch_model: True
  log_freq: 1
